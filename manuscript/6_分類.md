第六回: 分類
===============

「データサイエンス入門」第六回は分類問題を扱います。分類問題とは、データの特性に基づいて、データをいくつかのクラスに分類する問題です。例えば、ある機械のセンサーの値から、その機械が故障しているかどうかを判断したり、ある人の年齢や性別、職業などの情報から、その人がどのような商品を購入するかを予測する問題を扱います。分類問題は前回の回帰分析と同様、データサイエンスの基本的な問題の一つです。ここでは分類問題の基礎として、ロジスティック回帰を例に説明を行います。

## 二値分類

まず、分類問題の基礎として、二値分類を考えます。二値分類では、データを二つのクラスに分類します。予測したいのは機械が故障しているか、そうでないか、あるいは、ある人が商品を購入するか、しないか、といった具合です。ロジスティック回帰は、二値分類のための手法となります。

### ロジスティック回帰

ロジスティック回帰は、回帰分析と同様に、データの特性から、データがどのクラスに属するかを予測する手法です。二値分類を可能にするために、ロジスティック回帰では、データがあるクラスに属する確率を予測します。例えば、気温や降水量、気圧などのデータから、明日、雨が降るかどうかを確率的に予測することができます。雨ならば1、雨以外の天気は0として、明日は雨が降る確率を予測します。閾値、例えば0.5を設定し、この確率が0.5を超えた場合には雨が降ると判断します。

ロジスティック回帰モデルをR言語で実装してみましょう。ここでは気象庁から取得した、複数地点での2022年の5月の気象データを用意しました。データはすでにRにデータフレームとして読み込まれていることを想定します。

```r
dplyr::glimpse(df_weather)
#> Rows: 124
#> Columns: 4
#> $ pressure    <dbl> 1010.3, 1012.0, 1018.5, 1021.3, 1021.5, 1018.9, 1015.2, 1016.8, 1016.6, 1017.3, 1016.7, 1012.7, 1005.9, 100…
#> $ humidity    <dbl> 67, 59, 47, 63, 70, 67, 68, 51, 64, 66, 84, 96, 100, 76, 59, 70, 69, 50, 56, 62, 74, 65, 59, 67, 71, 82, 66…
#> $ temperature <dbl> 16.1, 15.2, 15.6, 17.0, 18.3, 20.0, 20.4, 18.4, 15.2, 17.5, 19.3, 18.8, 18.9, 20.6, 17.2, 17.5, 17.6, 19.6,…
#> $ weather     <fct> 雨, 雨, 雨以外, 雨以外, 雨以外, 雨以外, 雨以外, 雨以外, 雨, 雨, 雨, 雨, 雨, 雨以外, 雨, 雨以外, 雨以外, 雨…
```

weatherの列に「雨」か「雨以外」かが記録されています。このほかの気圧 pressure、湿度 humidity、気温 temperatureの情報をもとに、weatherを予測するモデルを作成します。ロジスティック回帰モデルは`glm()`関数を用いて作成します。`glm()`関数の`family`引数に`binomial`を指定することで、ロジスティック回帰モデルが実装されます。関数名や異なる引数の指定がありますが、予測対象のweatherを目的変数に、それ以外の変数を説明変数に指定する点は、回帰分析と同様です。

```r
# ロジスティック回帰モデルの作成
model <- 
  glm(weather ~ temperature + humidity + pressure, data = df_weather, family = binomial)
summary(model)
# 「雨」を1、「雨以外」を0として扱う
contrasts(df_weather$weather)
```

`summary()`関数で作成されたモデルの概要を確認します。ここでまず重要なのは回帰分析と同様、説明変数に対する係数です。前回の重回帰モデルで得られた係数の解釈と同様に、係数の値が正の場合は、説明変数が増加すると、目的変数の値も増加することを意味します。逆に、係数の値が負の場合は、説明変数が増加すると、目的変数の値は減少することを意味します。今回は目的変数を「雨」か「雨以外」と文字としてモデルを構築しましたが、Rでは文字を自動的に数値に変換しています。`weather`列の「雨」は1、「雨以外」は0として扱われています。そのため、目的変数が増加することは「雨」になることを意味します。

改めて係数の値を確認してみましょう。ここでは、係数の値が正の説明変数は、湿度 humidity のみです。湿度が上昇すると、雨が降る確率が上昇することを意味します。逆に、係数の値が負の説明変数は、気温と気圧の2つです。モデルでは、これらが上昇すると、雨が降る確率が下がることを示唆します。

この傾向をグラフを使って確認しましょう。次の図は湿度と天気、雨かどうかの散布図です。湿度が高いデータポイントでは雨になりやすいことがわかります。

```r
# 散布図を描画し、データの傾向を確認
library(ggplot2)
p <- ggplot(df_weather) +
  aes(humidity, as.numeric(weather)-1) +
  geom_point() +
  ylab("weather")
p
```

この図に、ロジスティック回帰モデルの予測結果を重ねてみましょう。次のコードでは、`geom_smooth()`関数を用いて、モデルの予測結果を描画しています。

```r
p + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "#57467b")
```

この曲線からも、湿度の増加が雨の確率を高めることを確認できます。

最後に、モデルに対して、新しいデータを与えて予測を行いましょう。
これにより、未知のデータに対するモデルの挙動を知ることができます。
weather以外の変数からなるデータフレームを用意し、`predict()`関数に与えて実行します。

```r
# 新しいデータをもとに「雨」の確率を求める
new_weather <- data.frame(temperature = 14.1, humidity = 88, pressure = 1001)
predicted_prob <- predict(model, newdata = new_weather, type = "response")
predicted_prob
```
ここで出力されるのは、モデルが「雨」と判定した際の確率です。
気温14.1、湿度88、気圧1001のデータを与えた場合、モデルは「雨」になる確率を99%と判定しています。
「雨以外」の確率を求めたい場合、次のように1から予測結果を引くことで求めることができます。
また別のデータについて予測を行いましょう。

```r
# 1- にすることで「雨以外」の確率を求める
1 - predict(model, 
        newdata = data.frame(temperature = 22.6, humidity = 28, pressure = 1023), 
        type = "response")
```

今度は「雨以外」の確率です。気温22.6、湿度28、気圧1023のデータを与えた場合、モデルは「雨以外」になる確率を99%と判定しています。

以上で二値のデータを分類するためのロジスティック回帰モデルの説明を終わります。二値以上のデータに分類する、多クラス分類と呼ばれる手法においても、ロジスティック回帰モデルは有効です。候補となるクラスの数だけ、ロジスティック回帰モデルを作成し、最も確率が高いクラスを予測結果として採用することで、多クラス分類を実現できます。一方、ロジスティック回帰モデルは、データの分類において、他の手法と比較して、精度が低いという欠点もあります。その理由の一つに、線形関係でしか分類できないという点が挙げられます。これに対しては、サポートベクターマシンやニューラルネットワークなどの手法が有効となります。

以上で、データサイエンス入門の第六回を終了します。ご視聴ありがとうございました。